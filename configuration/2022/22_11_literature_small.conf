// --- UPDATE THIS --- //
spark-settings.write-mode = "ignore"
data_version = "22.11"
chembl_version = "31"
ensembl_version = "108"
efo_version = "v3.47.0"
evidences.data-sources-exclude = ["ot_crispr", "encore", "ot_crispr_validation"]
# update defaults for next release
etl-dag.resolve = false
// --- END - UPDATE THIS --- //
target.input.chemical-probes.path = "gs://open-targets-pre-data-releases/22.11/input/target-inputs/chemicalprobes/chemicalprobes.json.gz"
literature.processing.abstracts.path = "gs://ot-team/jarrod/literature/Abstracts/*.parquet"
literature.processing.abstracts.format = "parquet"
literature.processing.full-texts.path = "gs://ot-team/jarrod/literature/Full-text/small-ft/*.parquet"
literature.processing.full-texts.format = "parquet"
common.additional-outputs = []
common.output-format = "parquet"

literature {

  // Bucket name information
  output = ${common.output-base_path}"/literature"

  // Release information
  epmcReleaseNumber = "22.01"

  processing {
    outputs = {
      raw-evidence {
        format = ${common.output-format}
        path = ${literature.output}"/rawEvidence"
      }
      cooccurrences {
        format = ${common.output-format}
        path = ${literature.output}"/cooccurrences"
      }
      matches {
        format = ${common.output-format}
        path = ${literature.output}"/matches"
      }
      literature-index {
        format = ${common.output-format}
        path = ${literature.output}"/literatureIndex"
      }
    }
  }

  embedding {
    outputs = {
      model = {
        path = ${literature.output}"/W2VModel"
      }
      training-set = {
        path = ${literature.output}"/trainingSet"
      }
    }
  }

  vectors {
    output {
      path = ${literature.output}"/vectors"
    }
  }

  evidence {
    output = {
      path = ${literature.output}"/evidence"
    }
  }
}

spark-settings.default-spark-session-config += {k: "spark.sql.shuffle.partitions", v: "960"}
spark-settings.default-spark-session-config += {k: "spark.default.parallelism", v: "960"}
spark-settings.default-spark-session-config += {k: "spark.executor.cores", v: "8"}
spark-settings.default-spark-session-config += {k: "spark.dynamicAllocation.maxExecutors", v: "60"}
spark-settings.default-spark-session-config += {k: "spark.dynamicAllocation.minExecutors", v: "1"}
spark-settings.default-spark-session-config += {k: "spark.sql.files.maxRecordsPerFile", v: "100000"}
